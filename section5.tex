\documentclass[11pt]{article}
\usepackage[margin=1 in, letterpaper]{geometry}
\usepackage{fontspec, graphicx, amsmath, amssymb, amsthm, array, physics, enumitem, cancel, multicol, float, dsfont}
\usepackage{latexsym, marvosym}
\usepackage[dvipsnames]{xcolor}

\setmainfont{Linux Libertine O}
\setsansfont{Linux Biolinum O}
\setmonofont{Latin Modern Mono}
\setmathrm{Latin Modern Math}
\theoremstyle{definition}
\newtheorem{theo}{\color{Maroon} Theorem}[section] 
\newtheorem{defin}[theo]{\color{Maroon} Definition}
\newtheorem{example}[theo]{\color{Maroon} Example}
\newtheorem{prob}[theo]{\color{Maroon} Problem}
% \newtheorem{example}[section]{\color{Maroon} Example}

\theoremstyle{remark}
\newtheorem*{soln}{\color{Maroon} Solution}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Bin}{\text{Bin}}
\newcommand{\Bern}{\text{Bern}}

%%%%%%%%%%%%%%%%%%% Statistics %%%%%%%%%%%%%%%%%%%%
\newcommand{\E}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left[ #1 \right]}
\newcommand{\cov}[2]{\textnormal{Cov}\left[ #1, #2 \right]}
\renewcommand{\var}[1]{\textnormal{Var}\left[ #1 \right]}
\newcommand{\Unif}{\textnormal{Unif}}
\newcommand{\Norm}{\mathcal{N}}
\newcommand{\ind}[1]{\mathds{1}_{#1}} 
\newcommand{\NBin}{\textnormal{NBin}}
\newcommand{\Geom}{\textnormal{Geom}}
\newcommand{\FS}{\textnormal{FS}}
\newcommand{\HGeom}{\textnormal{HGeom}}
\newcommand{\Pois}{\textnormal{Pois}}
\newcommand{\Expo}{\textnormal{Expo}}
\newcommand{\Beta}{\textnormal{Beta}}
\newcommand{\Gam}{\textnormal{Gamma}}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
    \def\independenT#1#2{\mathrel{\setbox0\hbox{$#1#2$}%
    \copy0\kern-\wd0\mkern4mu\box0}} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\inserttitle}{Section 05}
\newcommand{\insertauthor}{Max Guo \& Seung Hwan An}
\newcommand{\insertcourse}{STAT 110}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{fancyhdr}
\setlength{\headheight}{15pt}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[C]{\thepage}
\fancyhead[L]{\inserttitle}
\fancyhead[R]{\insertauthor}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

{\noindent\Huge\bf  \\[0.1\baselineskip] {\inserttitle }}\\[2\baselineskip]
{{\bf \insertcourse}\\ {\textit{October 18, 2021}}} \hfill {\large \textsc{\insertauthor}}
\smallskip

\hfill \noindent \textit{Credits to William Chen and Sebastian Chiu}


\section{Midterm Review}

We give the high level ideas of each problem after a quick solve through the problems. 

\begin{description}
\item[Problem 1a.] Use LOTP and Bayes' Rule. Recognize that $C$ and $D$ are complementary events. 
\item[Problem 1b.] Use LOTP (Conditional Form) and use the conditional independence of $T_1$ and $T_2$ given $C$ and $D$ to simplify $P(T_2 | T_1, C) = P(T_2 | C)$, etc. Then use the results from $1a$.
\item[Problem 2a.] Use Chicken-Egg story to recognize independence of $V$ and $W$, then use the PMF (or CDF) of Poisson (and complementary counting) to obtain $P(V \le 2)$.
\item[Problem 2b.] Recognize that $N = 6$ and use the PMF for Geometric, multiplied by the probability that $V = 2$ and $W = 4$ given $N = 6$ (directly calculate probability).
\item[Problem 2c.] Recognize that we must have $t = i + j + k$ for positive probability, and then using naive definition of probability we count successful possibilities by choosing $i$ things from $d$, $j$ things from $m$, and $k$ things from $n$, and total number of possibilities by choosing $i + j + k$ from $v$.
\item[Problem 3a.] This is three First Success distributions, and we sum the means by linearity of expectation.
\item[Problem 3b.] Use indicator random variables and count the number of sequences ($108$) and the probability of $RGB$ appearing as a sequence ($1/27$).
\item[Problem 4a.] Note that $\sum_{j=1}^n P(X = j) = 1$, then use the first of the formulas to solve for $c$.
\item[Problem 4b.] For $E[X]$, use the definition and plug in $c$ from before. For $\var X$, use $\var X = E[X^2] - E[X]^2$ and use LOTUS. 
\item[Problem 4c.] Note casework! For $x < 1$, we have $F(x) = 0$. For $x \ge n$, we have $F(x) = 1$. For everything in between, we sum $\sum_{j=1}^{\lfloor x \rfloor} P(X = j)$ and use the formulas.
\end{description}


\section{Continuous Random Variables}
\begin{description}
\item[What is a Continuous Random Variable (CRV)?] A continuous random variable can take on any possible value within a certain interval (for example, [0, 1]), whereas a discrete random variable can only take on variables in a list of countable values (for example, all the integers, or the values 1, $\frac{1}{2}, \frac{1}{4}, \frac{1}{8}$, etc.)
\item[Do Continuous Random Variables have PMFs?] No. The probability that a continuous random variable takes on any specific value is 0.
\item[How to I find the probability that a CRV takes on a value within an interval?] Use the CDF (or the PDF, see below). To find the probability that a CRV takes on a value in the interval $[a, b]$, subtract the respective CDFs.
\[P(a \leq X \leq b) = P(X \leq b) - P(X \leq a) = F(b) - F(a)\]
\item[What is the Cumulative Density Function (CDF)?] It is the following function of $x$.
		\[F(x) = P(X \leq x)\]
		With the following properties:
		\begin{enumerate}
		    \item $F$ is increasing.
		    \item $F$ is right-continuous.
		    \item $F(x) \rightarrow 1$ as $x \rightarrow \infty$, $F(x) \rightarrow 0$ as $x \rightarrow -\infty$
		\end{enumerate}

\item[What is the Probability Density Function (PDF)?] The PDF, $f(x)$, is the derivative of the CDF. 
\[ F'(x) = f(x) \]
Or alternatively,
\[ F(x) = \int_{-\infty}^x f(t)dt \]
Note that by the fundamental theorem of calculus,
\[ F(b) - F(a) = \int^b_a f(x)dx \]
Thus to find the probability that a CRV takes on a value in an interval, you can integrate the PDF, thus finding the area under the density curve.

Two additional properties of a PDF:  it must integrate to 1 (because the probability that a CRV falls in the interval $[-\infty, \infty]$ is 1, and the PDF must always be nonnegative.
\[\int^\infty_{-\infty}f(x)dx = 1 \hspace{2 cm} f(x) \geq 0\]
\item[How do I find the expected value of a CRV?] Where in discrete cases you sum over the probabilities, in continuous cases you integrate over the densities.
\[E(X) = \int^\infty_{-\infty}xf(x)dx \]
Review: Expected value is \emph{linear}. This means that for \emph{any} random variables $X$ and $Y$ and any constants $a, b, c$, the following is true:
\[E(aX + bY + c) = aE(X) + bE(Y) + c\]
\end{description}

\section{Discrete versus Continuous}
\begin{table}[H] \begin{center}
	\begin{tabular}{ccc}
	\hline
		~ & \textbf{Discrete} & \textbf{Continuous} \\ \hline
		$P(X \leq x) = $  & $F(x)$ (CDF) & $F(x)$ (CDF) \\ 
		To find probabilities, & Add over PMF $P(X = x)$ & Integrate over PDF $f(x) = F'(x)$ \\ 
		$E(X) =$ & $\sum_x xP(X=x)$ & $\int_{-\infty}^{\infty}xf(x)dx$ \\ 
		$E(g(X)) =$ & $\sum_x g(x)P(X=x)$ (LOTUS Discrete) & $\int_{-\infty}^{\infty} g(x)f(x)dx$ (LOTUS Continuous)
	\end{tabular}\end{center}
\end{table}

\section{Law Of The Unconscious Statistician (LOTUS)}
\begin{description}
\item[How do I find the expected value of a function of a random variable?]
Normally, you would find the expected value of X this way:
\[E(X) = \Sigma_x xP(X=x) \]
\[E(X) = \int^\infty_{-\infty}xf(x)dx \]
LOTUS states that you can find the expected value of a \emph{function of a random variable} g(X) this way:
\[E(g(X)) = \Sigma_x g(x)P(X=x) \]
\[E(g(X)) = \int^\infty_{-\infty}g(x)f(x)dx \]
\item[What's a function of a random variable?] A function of a random variable is also a random variable. For example, if $X$ is the number of bikes you see in an hour, then $g(X) =  2X$ could be the number of bike wheels you see in an hour. Both are random variables.
\item[What's the point?] You don't need to know the PDF/PMF of $g(X)$ to find its expected value. All you need is the PDF/PMF of $X$. 
\end{description}


%\section{Variance}
%\begin{description}
%\item[What is Variance?] Variance is the expected squared distance away from the mean. It is the square of the standard deviation.
%\[\var(X) = E((X - EX)^2) = E(X^2) - E(X) \]
%Oftentimes it's easier to calculate $E(X^2) - E(X)$ rather than $E(X - EX)$. You can find $E(X^2)$ with LotUS.

%The following is true for constants $a, b$:
%\[\var(aX + b) = a^2\var(X)\]
%The following is true \textbf{only if X and Y are independent}:
%\[\var(X + Y) = \var(X) + \var(Y)\]
%\end{description}

\section{Variance, Expectation and Independence}
\[\var X = E(X^2) - [E(X)]^2\]
If $X$ and $Y$ are independent, then
\[E(XY) = E(X)E(Y)\]


\section{Universality of Uniform} When you plug any random variable into its own CDF, you get a Uniform[0,1] random variable. When you put a Uniform[0,1] into an inverse CDF, you get the corresponding random variable. For example, let's say that a random variable X has a CDF
	\[ F(x) = 1 - e^{-x} \]
	By the Universality of the the Uniform, if we plug in X into this function then we get a uniformly distributed random variable.
	\[ F(X) = 1 - e^{-X} \sim U\]
	Similarly, since $F(X) \sim U$ then $X \sim F^{-1}(U)$. The key point is that \emph{for any continuous random variable X, we can transform it into a uniform random variable and back by using its CDF.}

\section{Uniform Distribution (Continuous)}
\begin{description}
\item Let us say that $U$ is distributed $\Unif(a, b)$. We know the following:
\begin{description}
	\item[Properties of the Uniform] For a uniform distribution, the probability of an draw from any interval on the uniform is proportion to the length of the uniform. The PDF of a Uniform is just a constant, so when you integrate over the PDF, you will get an area proportional to the length of the interval.

    \item[Transformable] For any $X \sim \Unif(a,b)$, we can represnt it as $X = (b-a) Y + a$, where $Y \sim \Unif(0,1)$. 

	\item[Example] Max throws darts really badly, so his darts are uniform over the whole room because they're equally likely to appear anywhere. Max's darts have a uniform distribution on the surface of the room. The uniform is the only distribution where the probability of hitting in any specific region is proportion to the area/length/volume of that region, and where the density of occurrence in any one specific spot is constant throughout the whole support.
	\item[PDF and CDF] 
\begin{eqnarray*}
\Unif(0, 1)
  \hspace{.7 in}
   f(x) = \left\{
     \begin{array}{lr}
       1 & x \in [0, 1] \\
       0 &  x \notin [0, 1]
     \end{array}
   \right.
   \hspace{.95 in}
   F(x) = \left\{
     \begin{array}{lr}
       0 & x < 0 \\
       x & x \in [0, 1] \\
       1 &  x > 1
     \end{array}
   \right.\\
\Unif(a, b)
  \hspace{.65 in}
   f(x) = \left\{
     \begin{array}{lr}
       \frac{1}{b-a} & x \in [a, b] \\
       0 &  x \notin [a, b]
     \end{array}
   \right.
   \hspace{.75 in}
   F(x) = \left\{
     \begin{array}{lr}
       0 & x < a \\
       \frac{x-a}{b-a} & x \in [a, b] \\
       1 &  x > b
     \end{array}
   \right. 
\end{eqnarray*}

\end{description}
\end{description}

\section{Normal Distribution (Continuous) (aka Gaussian)}
\begin{description}

\item Let us say that $X$ is distributed $\N(\mu, \sigma^2)$. We know the following:
\begin{description}
	\item[Central Limit Theorem] The Normal distribution is ubiquitous because of the central limit theorem, which states that averages of independent identically-distributed variables will approach a normal distribution regardless of the initial distribution.
	\item[Transformable] Every time we stretch or scale the normal distribution, we change it to another normal distribution. If we add $c$ to a normally distributed random variable, then its mean increases additively by $c$. If we multiply a normally distributed random variable by $c$, then its variance increases multiplicatively by $c^2$. Note that for every normally distributed random variable $X \sim \Norm(\mu, \sigma^2)$, we can transform it to the standard $\Norm(0, 1)$ by the following transformation:
	\[\frac{X - \mu}{\sigma} \sim \N(0, 1) \]
	\item[Example] Heights are normal. Measurement error is normal. By the central limit theorem, the sampling average from a population also converges to normal.
	\item[Standard Normal] - The Standard Normal, denoted $Z$, is $Z \sim \Norm(0, 1)$
	\item[PDF]
\[ f(x)=\frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2 \sigma^2}} \]
	\item[CDF] - It's too difficult to write this one out, so we express it as the function $\Phi(x)$
\end{description}
\end{description}

\section{Continuous Distributions}
\begin{center}
\renewcommand{\arraystretch}{2}
\begin{tabular}{cccccc}
\textbf{Distribution} & \textbf{PDF and Support} & \textbf{E(X)}  & \textbf{Variance}  &\textbf{Equivalent To}\\
\hline

\shortstack{Uniform \\ \Unif($a, b$)} & \shortstack{$ f(x) = \frac{1}{b-a}, x \in [a, b] $ \\$ f(x) = 0, x \notin [a, b]$} & $\frac{a+b}{2}$ & $\frac{(b-a)^2}{12}$ & $F_X(X)$ \\
\hline
\shortstack{Normal \\ $\N(\mu, \sigma^2)$} & $f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2 \sigma^2}}$ & $\mu$  & $\sigma^2$ & \\
\hline

\end{tabular}
\end{center}

\section{Poisson Distribution (Discrete)}
\begin{description}
\item Let us say that $X$ is distributed $\Pois(\lambda)$. We know the following:
\begin{description}
	\item[Story] There are rare events (low probability events) that occur many different ways (high possibilities of occurences) at an average rate of $\lambda$ occurrences per unit space or time. The number of events that occur in that unit of space or time is $X$.
	
	\item[Example] A certain busy intersection has an average of 2 accidents per month. Since an accident is a low probability event that can happen many different ways, the number of accidents in a month at that intersection is distributed $\Pois(2)$. The number of accidents that happen in two months at that intersection is distributed $\Pois(4)$
	
	\item[PMF] The PMF of a Poisson is:
\[P(X = k) = \frac{e^{-\lambda}\lambda^k}{k!}\]
\end{description}
\end{description}

\section{Discrete Distributions}
\begin{center}
\renewcommand{\arraystretch}{2}
\begin{tabular}{cccccc}
\textbf{Distribution} & \textbf{PMF and Support} & \textbf{E(X)}  & \textbf{Variance} & \textbf{Equivalent To}\\
\hline
\shortstack{Bernoulli \\ \Bern($p$)} & \shortstack{$P(X=1) = p$ \\$ P(X=0) = q$} & $p$ & $pq$ & $\Bin(1, p)$ \\
\hline
\shortstack{Binomial \\ \Bin($n, p$)} & \shortstack{$P(X=k) = {n \choose k}p^k(1-p)^{n-k}$  \\ $k \in \{0, 1, 2, \dots n\}$}& $np$ & $npq$ & Sum of $n$ Bern($p$) \\
\hline
\shortstack{Geometric \\ \Geom($p$)} & \shortstack{$P(X=k) = q^kp$  \\ $k \in \{$0, 1, 2, \dots $\}$}& $\frac{q}{p}$ & $\frac{q}{p^2}$ & \NBin($1, p$)\\
\hline
\shortstack{First Success \\ $\FS(p)$} & \shortstack{$P(X=k) = q^{k-1}p$  \\ $k \in \{$1, 2, 3, \dots $\}$}& $\frac{1}{p}$ & $\frac{q}{p^2}$ & \Geom(p) + 1\\
\hline
\shortstack{Negative Binomial \\ \NBin($r, p$)} & \shortstack{$P(X=n) = {n+r - 1 \choose r -1}p^rq^n$ \\ $n \in \{$0, 1, 2, \dots $\}$} & $r\frac{q}{p}$ & $r\frac{q}{p^2}$ &  Sum of $r$ Geom($p$)\\
\hline
\shortstack{Hypergeometric \\ \HGeom($w, b, n$)} & \shortstack{$P(X=k) = \frac{{w \choose k}{b \choose n-k}}{{w + b \choose n}}$ \\ $k \in \{$0, 1, 2, \dots $\}$} & $n\frac{w}{b+w}$ &&  \\
\hline
\shortstack{Poisson \\ \Pois($\lambda$)} & \shortstack{$P(X=k) = \frac{e^{-\lambda}\lambda^k}{k!}$ \\ $k \in \{$0, 1, 2, \dots $\}$} & $\lambda$ & $\lambda$ &  \\

\end{tabular}
\end{center}





\section*{Practice Problems}
\vskip .2 in

\begin{prob} \textbf{CDF of a Normal} \ \ (BH 5.33)

\medskip

Let $Z \sim \Norm(0,1)$. find $E(\Phi(Z))$ \textit{without} using LOTUS, where $\Phi$ is the CDF of $Z$.\\

% \vspace{0.5 in}
\fbox{\parbox{0.9 \textwidth}{
By Universality of the Uniform, $F(X) \sim \Unif(0,1)$ for any continuous random variable $X$ with CDF $F$. Since $Z \sim \N(0,1)$ and $\Phi$ is the CDF of the Normal distribution, $\Phi(Z) \sim \Unif(0,1)$ and $E(U) = \frac{1}{2}$.}}
\end{prob}

\begin{prob} \textbf{Practice with CDFs} 

\medskip

The cumulative distribution function of the random variable $X$ is given by
$$F(x) = 1 - e^{-x^2} \hspace{10pt} (x>0)$$
\begin{enumerate}[label = (\alph*)]
    \item $P(X>2)$
    
    % \vspace{1 in}
    \fbox{\parbox{0.9 \textwidth}{
    Notice that a CDF gives the probability that a random variable takes on the values less than $x$. So, 
    
    $$P(X>2) = 1- P(X\leq 2) = 1 - F(2) = 1 - (1-e^{-4}) = e^{-4} $$}}
    
    \item $P(1<X<3)$
    
    % \vspace{ 0.5 in}
    
    \fbox{\parbox{0.9 \textwidth}{
    $$P(1<X<3) = F(3) - F(1) = (1-e^{-9}) - (1 - e^{-1}) = e^{-1} - e^{-9}$$
    }}
    
\end{enumerate} 
\end{prob}

\begin{prob} \textbf{Pocky} \ \ (BH 5.12)

\medskip

You and a friend are divvying up a stick of pocky. You break it into two pieces, at a uniformly random point. Find the CDF and the average length of the longer piece (the one that you want). 

\medskip

\fbox{\parbox{0.9 \textwidth}{
We can assume the units are chosen so that the stick has length $1$. Let $L$ be the length of the longer piece, and let the break point be $U \sim \Unif(0,1)$. For any $l \in [1/2, 1]$, observe that $L < l$ is equivalent to $U < l$ and $1-U < l$. This can be written as 
\[1-l < U < l\]
so we can calculate the CDF of $L$ as follows:

\[F_{L}(l) = P(L < l) = P(1-l < U < l) = 2l - 1\]

So $L \sim \Unif(1/2, 1)$ and $E(L) = 3/4$. Which should make sense since the longer piece can be any length between $1/2$ and $1$.}}

\end{prob}

\pagebreak

\begin{prob} \textbf{Pocky Round 2} 

\medskip

Suppose Max and Sonny have a stick of pocky of length 1, and they decide to make a single break in it at point $U$, where $U \sim \Unif(0,1)$. What is the expected length of the part of the pocky that contains the point $\frac{1}{3}$? \\

\fbox{\parbox{0.9 \textwidth}{
Let $g(x)$ be the function that takes the location of the break (x) and returns that length of the part of the stick that contains the $\frac{1}{3}$ point. Let's try to figure out what $g(x)$ looks like. If the break occurs at $\frac{1}{2}$ then the length of the part of the stick that contains $\frac{1}{3}$ is $\frac{1}{2}$. If the break occurs at $\frac{2}{3}$ then the length of the part of the stick that contains $\frac{1}{3}$ is $\frac{2}{3}$. 

\[ g(x) = \begin{cases} 
      1-x & x < \frac{1}{3} \\
      x & x > \frac{1}{3}\\
  \end{cases}
\]

We can now use LOTUS to find the expected value of $g(U)$ where $U \sim \text{Unif}(0,1)$.

$$E(g(U)) = \int_{0}^{1} g(x)f(x) dx$$

The PDF of $U$ is just 1, so

$$E(g(U)) = \int_{0}^{1} g(x) dx$$

Let's now break up the integral and substitute the correct values for $g(x)$. 

$$E(g(U)) = \int_{0}^{1/3} g(x)dx + \int_{1/3}^{1} g(x) dx = \int_{0}^{1/3} (1-x)dx + \int_{1/3}^{1} x dx$$

Now, we can evaluate the definite integrals. 

$$E(g(U)) = (x - \frac{x^2}{2}) \bigg\rvert_{0}^{1/3} + \frac{x^2}{2} \bigg \rvert_{1/3}^1 = \frac{1}{3} - \frac{1}{18} + \frac{1}{2} - \frac{1}{18} = \frac{13}{18}$$.

Thus, the expected value of the pocky with the point $\frac{1}{3}$ is $\frac{13}{18}$}}.

\end{prob} 


\end{document}